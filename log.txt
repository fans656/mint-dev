2015-10-20 09:11:31
两个 link-layer switch (链路层交换机) 能彼此连接吗？
昨儿晚上看wiki说是switch根据MAC-端口映射来转发包，...
google了下，说是可以
之前以为不可以的，这么看来，switch有这样的功能：即不认识的MAC都从特定端口转发出去，之后就由这个端口连接的另一个switch来负责了

2015-10-20 10:26:36
[sim]
https://en.wikipedia.org/wiki/Collision_domain
) 模拟两台直连电脑间的collision
) 模拟三台通过hub(集线器)连接的..的collision

[learn]
https://en.wikipedia.org/wiki/Carrier_sense_multiple_access_with_collision_detection
CSMA/CD

2015-10-23 16:11:08
    sock.listen(backlog)
里头的backlog的含义有必要澄清，这个数是操作系统允许积压的尚未被accept的所有connection request(在这里就是一个SYN包)的最大个数
它说的不是服务器最多能维持几个tcp链接，而是说假如你accept得不够快的话，os最多帮你bookkeeping多少个链接请求

另外socket api里头分为 listen socket 和 data socket
其实在网络层面没这个区分，你 listen 在80就是80，这时还不存在connection，你只是跟os说了声“嘿，如果有人来找80，告我声啊”
(os是永远能收到发给任何端口的请求的，只是如果之前没有进程register在这个端口上，os就自己帮它回绝或者ignore了)
当请求到来的时候，只要有人监听，os就已经自作主张发回了ACK——不管你进程accept没有，这时其实connection就已经有了，在API层面由data socket代表，accept只是跟os取一下这个socket

2015-10-23 16:32:17
protocol定义了某一layer的两个entities之间的交互行为，两个entities可以逻辑上视为直接交互是因为下层layer提供的服务保证了它们正好看到自己层定义的协议
之前某处画的那个5层layer每层entities都交互实际上不准确，因为你看MAC层只能在双绞线两边儿交互，而IP层是确确实实穿过好多路由器交互的
所以更应该画成两边儿5层，中间经过几个3层
叫做53(年高考..模拟..囧)
4、5层直接交互，下面的是在每个hoop间交互的

2015-10-23 18:41:35
未来的学生会不会把Internet当作历史来学习；或者Internet只是更大的星际网络中一个古老的组成部分
连接地球和火星通信网的中间部分采用新的技术，使用新的协议

2015-10-24 14:31:39
关于带宽(bandwidth)和时延(latency)
设想一个传送带，这头儿有个搬运工往上放东西，他放的速度代表了带宽，即每秒能放上去多少字节的东西
传送带的速度以及距离决定了时延，即从这头儿放上去的东西需要多久能到那头儿

换个比喻，从一个城市到另一个城市的货运
我买了一条路，只能跑三轮儿，带宽相当小
你买了一条运河，大集装箱的轮船运，带宽极大

再来个，从地球到木星的链路，带宽不谈，时延大到简直了

最后一说：带宽就是圆柱体的截面

2015-10-26 19:17:24
computer networks (Andrew S. Tanenbaum) p86
这里说的很有意思，之前听说那些智能网络的概念还觉得对不对呀，是不是管得太多了？这里这个场景却确实很合适：
卫星通信是一种很便宜的broadcast——一个卫星的广播可以覆盖1/3个地球，把网络中cache的流量交给这种通信方式，点对点的交给线路通信
及至更细分的将流量类型与基础网络设施匹配的智能化调度，可以优化网络的使用

2015-10-28 19:01:45
https://www.grotto-networking.com/DiscreteEventPython.html
https://simpy.readthedocs.org/en/latest/
做到后面可能会用到simpy以及discrete event技术

2015-10-28 21:59:37
http://rpyc.readthedocs.org/en/latest/
想着用RPC来实现每个终端设备console中对终端的控制
python自带的xmlrpc弱爆了，object instance返回成dict
这个rpyc简直屌，完全是自己想象的样子，学名“transparent sysmmetric”

2015-10-29 12:34:21
感觉从GUI入手会蒙蔽逻辑，写得乱
打算下来先尝试用simpy做一个CLI的版本，然后再GUI化——GUI应该是独立于核心模拟的
也就是说不要GUI也能跑，而且可以有多个不同的GUI实现

2015-11-01 22:59:58
simpy必须用yield，没法二次封装，recv很难写
打算用线程重写，起手后发现不是那么简单——目前感觉至少相当于实现一个simpy的线程版

2015-11-02 09:43:03
有新想法了，琢磨了琢磨发觉用线程实现核心网(port, link)没有意义，做到最后还是各个线程间线性排队跑的，跟simpy一样
才意识到自己真正的需求只是api不要用yield，而这一点通过让用户写的entity逻辑代码跑在单独的线程中就可以
每个port自己维护一些threading.Event()
simpy里跑到相应的地方就给它们 e.set()
然后在用户代码里的wait()中，e.wait(); e.clear()
因为port到entity的关系是一对一的，所以不会有什么问题

2015-11-02 12:18:39
又思索了思索，意识到mint实际上需要支持两种模式
1) 实时网络
    开始运行后，所有线程模拟的那些硬件上，就已经有数据在不断地传输
    用户在GUI中点击某个电脑的send packet，另一边的数据流里，中间某处显出接收到的0101——前后都有线路噪声的0101
2) 逻辑网络
    所有网络中发生的行为都有精确的先后依赖
    比如可以保证pc1发送出1111的时候，链路上正好流过000的preamble

实时网络使用用户提供的entity线程、代码，所以不能用simpy的yield
逻辑网络可以使用simpy
但为了提供一致的接口(即用户代码中，用了mint的同步机制就是逻辑网络，不用就是实时网络)，所以只能使用线程

2015-11-04 11:52:35
之前写到hub就晕乎了，这些天来一直卡卡停停
昨儿跟肖师兄聊这个也聊了挺久
今儿终于搞定啦！精确时序，hub连3个host，啊哈哈哈

2015-11-04 13:13:56
描述一下目前的model:
In the first tik, what have the system done is allow all the entities to put the data (that they want to send) into the system's hardware world (i.e. ports).
Then comes the first tok.
The tok cosists of two phases: output and input.
In the output phase, ports pull the data given to them, set their output symbol accordingly (or '0' if there are no data given).
In the input phase, ports detect their peer's output symbol, convert into bits and push to their entity.

So a `recv` will block in the first tik, and only after the first tok, in the second tik will it be able to fetch one bit.

A tok will transfer one bit along a peered ports, a tik after it then will process this bit.

In the scene of links or hubs, their bit process is take one port's bit and give it to another port, we call this a "port pimp". The take requires one previous tok, and the give occured in one tik. These comprise into a full tik-tok step. So a bit travels along the network path will be delayed on every "port pimper" at least 1 tik-tok. If the pimper it self `wait` in the process, then the bit will be delayed more tik-tok. For example, in a system where a link with latency of 3 linking two hosts, the data transfer latency between the hosts will be 4.

2015-11-04 22:40:18
https://en.wikipedia.org/wiki/Consistent_Overhead_Byte_Stuffing
https://pypi.python.org/pypi/cobs
COBS - Consistent Overhead Byte Stuffing
    一种 framing protocol
    PPP 也会做 framing

https://en.wikipedia.org/wiki/High-Level_Data_Link_Control
https://www.quora.com/What-is-the-purpose-of-bit-stuffing-and-byte-stuffing-in-computer-communication
HDLC 用的是 bit stuffing
好像在asynchronous framming中也会用byte stuffing

https://networkengineering.stackexchange.com/questions/13233/bit-and-byte-stuffing-in-hdlc-frames/13281#13281?newreg=c9fe3315235f42599c1823467da64d4d
看样子说两种stuffing的使用场景不同：
    bit stuffing - synchronous serial link
    byte stuffing - asynchronous serial link (e.g. RS-232)

2015-11-04 23:55:54
http://www.sangoma.com/tutorials/sync_n_async/
这里讲解了 sychronous/asynchronous communication

2015-11-05 08:24:06
https://networkengineering.stackexchange.com/questions/5263/why-is-the-ethernet-frame-size-fixed
这里讲了为啥Ethernet的frame有最小/最大长度限制
主要是由于collision detection的需要，CSMA/CD啥的
这个framing之后再做

https://networkengineering.stackexchange.com/questions/10423/how-long-is-a-frame-really-supposed-to-be-and-when-is-a-frame-a-babble
and mark

2015-11-05 19:29:43
https://en.wikipedia.org/wiki/Code
直接看海明码好晕啊，，一路溯源，从这里看起

https://en.wikipedia.org/wiki/Coding_theory
https://en.wikipedia.org/wiki/Forward_error_correction

https://en.wikipedia.org/wiki/Binary_erasure_channel
https://en.wikipedia.org/wiki/Binary_symmetric_channel
移除信道反而说是最简单的

https://en.wikipedia.org/wiki/Erasure_code
移除编码

https://en.wikipedia.org/wiki/Hamming_distance
汉明距离

2015-11-05 21:12:06
https://en.wikipedia.org/wiki/Block_code
一路到这里，终于能看明白点儿了
block code 就是把一个bit串分成长度固定的一块儿一块儿，然后每块儿通过查字典翻译成某个另外的bit块儿——叫做codeword

2015-11-06 15:10:17
感觉可以暂时略过物理层的error detection/correction
线路错误导致的丢包可以直接模拟(比如不管错没错，NIC丢掉收到的第2个frame)
——因为书里大约意思是CRC这种东西都是物理设备(NIC)来做的，OS根本看不到检错纠错的过程，它只能意识到包到没到层面的事件

2015-11-07 00:12:04
[undone]
http://www.tldp.org/LDP/khg/HyperNews/get/net/net-intro.html
这里最开始的一小段示例代码大约说明了网络通信中buffer空间不足时发生了什么：
通信面向的数据都是分块的(frame, packet, etc..)，当某个块到达时没有足够的buffer来保存它，就丢弃这个块
这样可以保证存起来的数据的integrity——如果只是在比特流中丢弃某些bit，可能造成数据的misinterpretation

2015-11-07 07:52:16
来梳理下理解：
physical layer (layer 1) 提供的服务是

    put_bits_for_send bits
        This put the `bits` in the NIC output buffer.
        As long as there are bits in the buffer,
        NIC will retrieve some bit/bits, encoding into a symbol,
        modulate and output the symbol.
        (Maybe 13 bits ouput parallelly every 1ns on a parallel port,
        which run down a parallel link, reach in another parallel port,
        then enter in a modem.
        Where the modem collect them into its buffer,
        and do it's (probably different) encoding and outputing work,
        say, take 3 bit every 1ms and encoding into a symbol which has
        8 value and modulate into voltage 0.2, 0.4, ..., 0.8 etc and
        output to a serial line.)
        If there are no valid data in the NIC buffer, NIC just keep its
        output symbol intact (maybe 13 zero bits).
    
    get_bits n_bits
        ...(不写了，这么写太费劲)

link layer 的功能是被分成两个部分，分别在OS中和NIC中完成的
OS负责添加control header，然后把 header+payload 的 bytes 交给NIC
NIC负责添加preamble/postamble，bit/byte stuffing，然后把处理后的bits发出去
(问题：bit/byte stuffing 不会影响到比如 hamming code 吗？)
(猜测：NIC的下层功能不管纠错，如果出错，只会影响到某些frame的识别：比如有些该识别出来的没识别到(丢包)，或者不该识别的识别了(这个会由link layer层的后续处理来检测这个frame的非法性))
设想NIC的真实处理过程：
OS请求send (link layer) packet (i.e. header + payload)
NIC通过DMA把OS请求发送的bytes copy到自己的buffer中，注意是从buffer第2个字节开始放的，第一个字节留给preamble
然后发送过程中才在硬件电路上实现bit stuffing
如果在发送过程中(准确说是尚没有可用buffer的时候)OS又请求发送了，那就让它等着——呃，其实准确来说是OS留个口信然后就去干别的事儿了，让NIC不忙了之后自己interrupt

然后是接收
NIC上会检测preamble来识别frame，把识别出来的放在buffer里——所以这里可能是一个frame有最大值限制的原因！因为buffer大小是有限的。
这里要保证buffer至少能容纳两个frame——一个已接收到的fa的和一个正在接收的fb
嗳，其实这么model更好：
NIC有一个有限容量的queue，还有一个容纳正在接收frame的buffer
当一个frame接收完，queue没满的话就把这个frame放进去，满了的话就把frame丢掉
大概意思是这样，实现上具体细节没想清楚——这里的关键是要意识到，线路上的电压是一直变化的，假如1ns是1bit的话，时间过了这个bit没存起来那就丢了

所以终于搞明白 fast sender / slow receiver 的含义了(excited!)：
一个来不及处理incomming frame的NIC是会丢掉来包的，那么sender发的太快也没用，反正这些包都无法被收到，发在网络上还占用网络资源
所以，我们需要flow control

顺便，也明白了slow sender的含义：NIC处理速度有限(OS想发快也不行，NIC没ready它就只能等着——虽然可以干别的事，但发送这茬只能等NIC ready了再进行，因为OS手头的memory也有限呀)，NIC连接的信道bandwidth有限，乃至OS进行协议处理，OS上层application当producer生产数据的速度有限
这些共同决定了sender的发送速度，叫做bandwidth - 56Kbps, 1Gbps, etc..
链路上的波特率可以等价转换成比特率，也是bandwidth
链路的bandwidth，NIC每秒处理字节数的bandwidth，OS协议处理handle的数据的bandwidth，等等，，这些中的最小值决定了sender最终的bandwidth

链路的长度决定了latency：1个bit从这端进去，多久从那端出来

2015-11-07 09:18:32
https://en.wikipedia.org/wiki/Data_link_layer
在看这个，然后忽然想到这么一个场景
a,b,c都连在一个hub或switch上
然后a发了一个frame，里头的source address是假的——是写的b的，destination address写的c的
完了b和c又正在通信，这么着会不会因为a的这个frame而干扰到b跟c的通信？
于是：

https://en.wikipedia.org/wiki/MAC_spoofing
先不看了，等模拟器做到IP层再说——主要是涉及到ARP，完了ARP是涉及IP的

2015-11-07 14:26:15
在重写模型，要确认一下python里list的append之类操作是不是thread safe的

2015-11-07 21:43:06
总是做一段就卡住，然后在费力的思索过后某时又灵光一现
这次是卡在了flow control，不知道怎么model模拟器才能支持fast sender/slow receiver的概念
刚才是在厕所突然意识到，
    
    @proc
    def _():
        a.send('111')
    
    @proc
    def _():
        print b.recv(8)

这样的function/thread们的实质是在定义网络的行为——或者说网络在时间序列中发生的non-trivial的事件(event)
比如这里定义的是：a在时刻0发送111，b从时刻0开始接收，8个tik-tok后，收到足够的bit，然后打印出来
我们可以改写：
    
    @proc
    def _():
        wait(3)
        a.send('111')
    
    @proc
    def _():
        wait(8)
        print b.recv(8)

这样定义的就是：a在时刻3发送111，b在时刻8开始接收——因为port经过8个tik-tok已经收到了足够的bits，所以recv立即返回
而在之前的定义中，recv在没有足够的bits到来时会自动跳到下一个时刻去继续接收

之前做的一个所有非daemon线程结束就将模拟结束的feature，其实质就是当proc定义的网络中的non-trivial event都完成后，就结束模拟

之后想做的那种通过真实application定义host行为的feature，相比现在这种只由单线程定义的host行为，要求更高——比如application中可以开启任意多的线程，甚至进程(此时通过rpyc来进程通信)
那么此时，network中的 worker threads 和 behaviour threads 就是彼此牵制
比如对于一个有限buffer的NIC(Network Interface Controller - 网卡)，host的send也可能block

http://stackoverflow.com/questions/5407182/blocking-sockets-when-exactly-does-send-return
(看，这里就说socket的send (TCP/UDP)可能block - due to lack of kernel send buffer)

这里block的实质跟recv中是一样的，都是通过wait(0)来允许下一个tok的进行，从而到下一个tik中尝试仍然可能失败的接收

之前的model中，内容比较简单，所有的处理其实都是在behaviour thread中做的，而核心网中的port又混杂了一些本应是NIC该做的工作
现在想把port简化到只包含最基本的 output/input symbol 的地步
然后NIC、OS部分的工作放在 worker threads 中——现在是支持一个host多个线程的

NIC和OS的分工是这样的：
NIC只负责bit stuffing和premable/postamble的添加
其余都在OS里做
这意味着link layer是横跨NIC和OS两个部分的——书中也是这么描述的
当然这只是一种人为的划分，比如TOE(TCP offload engine [1])中就把整个TCP/IP stack放在了NIC中

[1]
https://en.wikipedia.org/wiki/TCP_offload_engine

这里的关键是，NIC中有几个线程在跑(具体有几个、分别干什么都还没有想清楚，不过这不是重点，按mint的设计思路，这些都是可以由用户决定的——用户如果愿意，专门开一个线程跑while True也没问题)，OS也有一堆
这要模拟的就是真实的PC机——但是现在自己的问题是... 对操作系统这片迷雾重重的虚拟层，想不明白呀~~

2015-11-11 11:22:52
感觉NIC和OS之间又有线程乱序的问题了：如果两者都是worker thread,
那么在一个tik-tok种可能NIC先pull了frame，然后OS才给NIC发数据

于是似乎应该给所有network掌管的线程都添加一个优先级
actor 最高
os 次之
worker 最次

其实worker的真实含义是 port operator，而且它们在一次tik-tok中要运行两个phase
一个是向port写，另一个是从port读
读一定要严格发生在写之后，这样才能保证能读到初始数据

但是又一想目前阶段似乎不需要做优先级
把OS model到actor上就行了

这样，fast sender 就是一个有大buffer的NIC，配合一个一口气塞满buffer的actor
此后NIC就以最高速率把frame一个接一个地扔到physical link上
slow receiver 呢，就是一个只有一点儿可怜小buffer(比如只能容纳一个frame)的NIC，配合一个recv一个frame之后处理了很久(比如wait(65536))的actor
这样，在actor下一次frame前，第二个frame之后的所有frame都会被丢弃
fast sender就想：咦，咋丢这么多，我真是日了狗了——然后哗哗哗又retransmit了一堆

下来就需要我们实现一个flow control的protocol，这个protocol效果如何呢，可以通过对比使用前和使用后在receiver的NIC上丢包事件的发生频度来衡量
conservative的protocol可能一个包都不丢，但效率不高
aggressive的protocol可能会丢一些包，不过相应地就会消耗更多的网络资源——那些因为receiver buffer不足而丢掉的数据都算作网络传输的overhead了

哦对了，顺便一提，当OS model到actor上时，因为network施加的限制，actor永远不会与worker同时运行，所以actor与worker线程间其实是不存在同步问题的——变量读写so easy!

2015-11-11 12:12:21
感觉NIC不该把byte转换成bits——效率太低了，直接在bytes上位操作输出输入bit估计效率更高些

2015-11-11 15:42:22
以上
但是这么debug起来挺方便的，以后性能成为瓶颈的时候再说吧，，位操作应该还挺麻烦的——主要是bit stuffing要死

现在NIC也做好了——就是说link layer中NIC负责的那部分包括
    framing (preamble/postamble)
    bit stuffing
以及NIC与OS的接口(send/recv)，与physical port的接口(send/recv)
都ok了

下来就可以做flow control啦
...是不是the little book of semaphores又要放了....
